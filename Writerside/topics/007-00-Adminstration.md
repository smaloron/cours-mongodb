# Administration

## R√©plicaction

Un Replica Set dans MongoDB est un groupe d'instances MongoDB qui maintiennent les m√™mes donn√©es pour assurer la haute disponibilit√© et la redondance. Il est compos√© de plusieurs n≈ìuds (serveurs), dont au moins un est un n≈ìud primaire et les autres sont des n≈ìuds secondaires.

### Composition d'un replica set

1. Un n≈ìud primaire (Primary)

   - C'est le serveur principal qui re√ßoit toutes les op√©rations d'√©criture et de lecture (sauf si la lecture secondaire est activ√©e).
   - Il synchronise automatiquement ses donn√©es avec les n≈ìuds secondaires.
   - Un seul n≈ìud primaire peut exister √† un instant donn√©.

2. Un ou plusieurs n≈ìuds secondaires (Secondary)

    - Ces n≈ìuds r√©pliquent les donn√©es du n≈ìud primaire en lisant son oplog (journal des op√©rations).
   - Ils servent principalement pour la redondance et la tol√©rance aux pannes.
   - Ils peuvent √™tre configur√©s pour r√©pondre aux requ√™tes en lecture si besoin.
      
3. (Optionnel) Un n≈ìud arbitre (Arbiter)

   - Il ne stocke pas de donn√©es mais participe au processus d'√©lection en cas de panne du n≈ìud primaire.
   - Il est utile pour √©viter une configuration avec un nombre pair de n≈ìuds, ce qui permet d'√©viter des conflits lors des √©lections.

### Fonctionnement d'un Replica Set

1. Lorsqu'une requ√™te d'√©criture arrive, elle est trait√©e par le n≈ìud primaire.
2. Les n≈ìuds secondaires r√©pliquent les modifications en copiant l'oplog du primaire. La r√©plication est asynchrone, mais g√©n√©ralement assez rapide.
3. Si le n≈ìud primaire tombe en panne, un vote d'√©lection est d√©clench√© entre les secondaires et un nouveau n≈ìud primaire est √©lu automatiquement.
4. Une fois le n≈ìud d√©faillant r√©par√©, il peut r√©int√©grer le Replica Set en tant que n≈ìud secondaire.

### Un exemple avec Docker

```yaml
# docker-compose.yml

services:
  mongo1:
    image: mongo:latest
    container_name: mongo1
    restart: always
    command: ["mongod", "--replSet", "rs0", "--bind_ip", "0.0.0.0"]
    ports:
      - "27020:27017"
    networks:
      - mongo-network
    volumes:
      - mongo1_data:/data/db

  mongo2:
    image: mongo:latest
    container_name: mongo2
    restart: always
    command: ["mongod", "--replSet", "rs0", "--bind_ip", "0.0.0.0"]
    ports:
      - "27021:27017"
    networks:
      - mongo-network
    volumes:
      - mongo2_data:/data/db

  mongo3:
    image: mongo:latest
    container_name: mongo3
    restart: always
    command: ["mongod", "--replSet", "rs0", "--bind_ip", "0.0.0.0"]
    ports:
      - "27022:27017"
    networks:
      - mongo-network
    volumes:
      - mongo3_data:/data/db

  mongo-arbiter:
    image: mongo:latest
    container_name: mongo-arbiter
    restart: always
    command: ["mongod", "--replSet", "rs0", "--bind_ip", "0.0.0.0"]
    networks:
      - mongo-network

  setup:
    image: mongo:latest
    container_name: mongo-setup
    depends_on:
      - mongo1
      - mongo2
      - mongo3
      - mongo-arbiter
    entrypoint: [ "bash", "-c", "sleep 10 && mongosh --host mongo1:27017 /scripts/replica-init.js" ]
    volumes:
      - ./scripts:/scripts
    networks:
      - mongo-network

networks:
  mongo-network:
    driver: bridge
volumes:
  mongo1_data:
  mongo2_data:
  mongo3_data:
```

**Nous avons ici 4 conteneurs**
- Trois conteneurs de donn√©es dont le premier sera le n≈ìud primaire.
- Un conteneur d'arbitrage qui ne servira qu'√† voter pour un nouveau n≈ìud primaire.
- Un conteneur `setup` qui lancera la configuration apr√®s un d√©lai pour laisser le temps aux autres conteneurs de s'instancier.

#### Comment √ßa marche

#### Configuration des n≈ìuds

```yaml
command: ["mongod", "--replSet", "rs0", "--bind_ip", "0.0.0.0"]
```

Chaque conteneur lance le service MongDB avec les options suivantes :

- `--replSet` indique que ce conteneur fait partie d'un replica set.
- `rs0`, il s'agit de l'identifiant du replica set.
- `"--bind_ip", "0.0.0.0"`, lie le conteneur √† toute interface r√©seau disponible. Cela permet aux conteneurs de communiquer entre eux.

#### Initialisation

Le conteneur `setup` d√©clare un entrypoint avec la commande suivante :

```yaml
entrypoint: [ "bash", "-c", "sleep 10 && mongosh --host mongo1:27017 /scripts/replica-init.js" ]
```

- `bash`, le programme √† lancer
- `-c`, indique que la chaine de caract√®re qui va suivre doit √™tre ex√©cut√©e comme une instruction. S'il y a plusieurs commandes dans cette cha√Æne elles doivent √™tre s√©par√©es par `&&`.
- `sleep 10`, d√©finit un d√©lai avant l'ex√©cution des commandes suivantes.
- `mongosh --host mongo1:27017 /scripts/replica-init.js`, lance un shell MongoDB sur le premier n≈ìud et ex√©cute un fichier javascript de configuration.

```javascript
rs.initiate({
    _id: "rs0",
    members: [
        { _id: 0, host: "mongo1:27017" },
        { _id: 1, host: "mongo2:27017" },
        { _id: 2, host: "mongo3:27017" },
        { _id: 3, host: "mongo-arbiter:27017", arbiterOnly: true }
    ]
});
```

### Test

<procedure>
<step>
Instancier toute la stack

```shell
docker compose up -d
```
</step>

<step>
Lancer un terminal sur le n≈ìud primaire

```shell
docker exec -it mongo1
```
</step>
<step>
V√©rifier la configuration du replica set

```mongodb
rs.status()
```
</step>
<step>
Effectuer une insertions

```mongodb
use test
```

```mongodb
db.persons.insertOne({name: 'Ada Lovelace'}
```
</step>

<step>
Sortir du shell Mongo

```mongodb
exit
```
</step>
<step>
En ouvrir un autre sur le conteneur mongo2

```shell
docker exec -it mongo2
```
</step>
<step>
V√©rifier que l'insertion a bien √©t√© r√©pliqu√©e

```mongodb
use test
```

```mongodb
db.persons.find({})
```
</step>
</procedure>

### Avantages de cette technique

- **Haute disponibilit√©** : Si un serveur tombe, un autre prend le relais.
- **Protection contre la perte de donn√©es** : La r√©plication garantit la redondance.
- **Scalabilit√© en lecture** : On peut lire depuis les secondaires pour all√©ger la charge.

## Sharding

Technique de partitionnement horizontal des donn√©es, permettant de distribuer la charge entre plusieurs serveurs afin d'am√©liorer les performances et la scalabilit√© d'une base de donn√©es volumineuse.

### Pourquoi
- **Scalabilit√© horizontale**¬†: Permet d'ajouter des n≈ìuds pour r√©partir la charge.
- **Gestion efficace des grosses bases de donn√©es**¬†: Divise les collections en morceaux (chunks) r√©partis sur plusieurs shards.
- **Disponibilit√© et tol√©rance aux pannes**¬†: Chaque shard peut √™tre r√©pliqu√© pour garantir la haute disponibilit√©.

### Comment

Un cluster "sharded" est compos√© des serveurs suivants :

- **Shards**¬†: Stockent les donn√©es r√©elles et peuvent √™tre des Replica Sets pour assurer la haute disponibilit√©.
- **Config Servers**¬†: Stockent la configuration du cluster et les m√©tadonn√©es des chunks.
- **Query Routers (mongos)**¬†: Servent d'interface entre l'application et le cluster, dirigeant les requ√™tes vers les bons shards.

```plantuml
@startuml

' D√©finition des √©l√©ments
node "Client Application" as client {
    
}
node "MongoDB Cluster" {

    node "Query Router" {
        [Mongo Query Router (mongos)]
    }
    
    node "Config Servers (configReplSet)" {
        [Config Server 1] 
        [Config Server 2] 
        [Config Server 3] 
    }

    node "Shard 1 (shard1ReplSet)" {
        [Shard1-1 Primary]
        [Shard1-2 Secondary]
        [Shard1-3 Secondary]
    }

    node "Shard 2 (shard2ReplSet)" {
        [Shard2-1 Primary]
        [Shard2-2 Secondary]
        [Shard2-3 Secondary]
    }
}

[client] -d-> [Mongo Query Router (mongos)]
' Connexions entre les √©l√©ments
[Mongo Query Router (mongos)] --> [Config Server 1]
[Mongo Query Router (mongos)] --> [Config Server 2]
[Mongo Query Router (mongos)] --> [Config Server 3]

[Mongo Query Router (mongos)] --> [Shard1-1 Primary]
[Mongo Query Router (mongos)] --> [Shard2-1 Primary]

[Shard1-1 Primary] --> [Shard1-2 Secondary]
[Shard1-1 Primary] --> [Shard1-3 Secondary]

[Shard2-1 Primary] --> [Shard2-2 Secondary]
[Shard2-1 Primary] --> [Shard2-3 Secondary]

@enduml

```

### Exemple de d√©ploiement avec Docker

- 2 shards (r√©pliqu√©s)
- 3 config servers
- 1 routeur (mongos)

```yaml
version: '3.8'

services:
   # Config Servers (stockent la config du cluster)
   configsvr1:
      image: mongo:6.0
      container_name: configsvr1
      command: mongod --configsvr --replSet configReplSet --port 27017
      ports:
         - 27017:27017
      networks:
         - mongo-cluster

   configsvr2:
      image: mongo:6.0
      container_name: configsvr2
      command: mongod --configsvr --replSet configReplSet --port 27017
      networks:
         - mongo-cluster

   configsvr3:
      image: mongo:6.0
      container_name: configsvr3
      command: mongod --configsvr --replSet configReplSet --port 27017
      networks:
         - mongo-cluster

   # Shard 1 - Replica Set
   shard1-1:
      image: mongo:6.0
      container_name: shard1-1
      command: mongod --shardsvr --replSet shard1ReplSet --port 27017
      networks:
         - mongo-cluster

   shard1-2:
      image: mongo:6.0
      container_name: shard1-2
      command: mongod --shardsvr --replSet shard1ReplSet --port 27017
      networks:
         - mongo-cluster

   shard1-3:
      image: mongo:6.0
      container_name: shard1-3
      command: mongod --shardsvr --replSet shard1ReplSet --port 27017
      networks:
         - mongo-cluster

   # Shard 2 - Replica Set
   shard2-1:
      image: mongo:6.0
      container_name: shard2-1
      command: mongod --shardsvr --replSet shard2ReplSet --port 27017
      networks:
         - mongo-cluster

   shard2-2:
      image: mongo:6.0
      container_name: shard2-2
      command: mongod --shardsvr --replSet shard2ReplSet --port 27017
      networks:
         - mongo-cluster

   shard2-3:
      image: mongo:6.0
      container_name: shard2-3
      command: mongod --shardsvr --replSet shard2ReplSet --port 27017
      networks:
         - mongo-cluster

   # Query Router (mongos)
   mongos:
      image: mongo:6.0
      container_name: mongos
      command: mongos --configdb configReplSet/configsvr1:27017,configsvr2:27017,configsvr3:27017 --port 27017
      ports:
         - 27020:27017
      networks:
         - mongo-cluster
      depends_on:
         - configsvr1
         - configsvr2
         - configsvr3
         - shard1-1
         - shard1-2
         - shard1-3
         - shard2-1
         - shard2-2
         - shard2-3

networks:
   mongo-cluster:
      driver: bridge

```

#### Configuration du cluster

Pour la configuration, nous cr√©ons un fichier bash

```bash
#!/bin/bash

## setup-sharding.sh

echo "‚è≥ Initialisation du cluster MongoDB shard√©..."

# Initialiser le Replica Set des Config Servers
echo "üöÄ Configuration des Config Servers..."
docker exec -it configsvr1 mongosh --eval '
rs.initiate({
  _id: "configReplSet",
  configsvr: true,
  members: [
    { _id: 0, host: "configsvr1:27017" },
    { _id: 1, host: "configsvr2:27017" },
    { _id: 2, host: "configsvr3:27017" }
  ]
});
'

# Attendre quelques secondes pour l'initialisation
sleep 5

# Initialiser le Replica Set du Shard 1
echo "üöÄ Configuration du Shard 1..."
docker exec -it shard1-1 mongosh --eval '
rs.initiate({
  _id: "shard1ReplSet",
  members: [
    { _id: 0, host: "shard1-1:27017" },
    { _id: 1, host: "shard1-2:27017" },
    { _id: 2, host: "shard1-3:27017" }
  ]
});
'

# Attendre quelques secondes pour l'initialisation
sleep 5

# Initialiser le Replica Set du Shard 2
echo "üöÄ Configuration du Shard 2..."
docker exec -it shard2-1 mongosh --eval '
rs.initiate({
  _id: "shard2ReplSet",
  members: [
    { _id: 0, host: "shard2-1:27017" },
    { _id: 1, host: "shard2-2:27017" },
    { _id: 2, host: "shard2-3:27017" }
  ]
});
'

# Attendre quelques secondes pour l'initialisation
sleep 5

# Ajouter les Shards au Query Router (mongos)
echo "üîó Ajout des shards au routeur..."
docker exec -it mongos mongosh --eval '
sh.addShard("shard1ReplSet/shard1-1:27017");
sh.addShard("shard2ReplSet/shard2-1:27017");
'

# Activer le sharding sur la base de donn√©es `mydatabase`
echo "‚úÖ Activation du sharding sur la base de donn√©es mydatabase..."
docker exec -it mongos mongosh --eval '
sh.enableSharding("mydatabase");
'

# V√©rifier l'√©tat du cluster
echo "üìú √âtat du cluster :"
docker exec -it mongos mongosh --eval 'sh.status();'

echo "üéâ Cluster MongoDB sharded configur√© avec succ√®s !"

```

Pour rendre le fichier ex√©cutable

```shell
chmod +x setup-sharding.sh
```

Pour lancer le script

```shell
./setup-sharding.sh
```

#### Pourquoi trois config servers ?

Un Config Server dans MongoDB stocke les m√©tadonn√©es du sharding (comme la r√©partition des chunks entre les shards). Il est essentiel au bon fonctionnement du cluster.

MongoDB exige au moins trois Config Servers pour garantir :

- **La haute disponibilit√© :**

Si un seul Config Server tombe en panne, le cluster continue √† fonctionner.
Avec un seul Config Server, une panne entra√Ænerait une perte de donn√©es de sharding.


- **La tol√©rance aux pannes :**

MongoDB utilise un Replica Set pour les Config Servers.
Avec 3 membres, m√™me si un serveur tombe, les deux autres peuvent continuer √† fonctionner.

- **La s√©curit√© des m√©tadonn√©es :**

Le sharding repose sur ces m√©tadonn√©es. Si elles sont corrompues ou perdues, le cluster devient instable.

Trois Config Servers assurent une r√©plication et une coh√©rence des informations.

- **√âviter les partitions r√©seau :**

Si un seul Config Server est utilis√© et qu‚Äôil devient injoignable, le mongos (Query Router) ne peut plus router les requ√™tes.

Avec trois Config Servers, un quorum est toujours maintenu.

#### R√®gles importantes pour les Config Servers
- Toujours avoir un nombre impair de Config Servers (ex : 3, 5, 7...) pour √©viter les conflits de quorum.
- Ne pas stocker des donn√©es utilisateur sur les Config Servers (ils sont uniquement destin√©s aux m√©tadonn√©es).
- Tous les Config Servers doivent √™tre en Replica Set (obligatoire depuis MongoDB 3.4).




